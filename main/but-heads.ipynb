{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from pos_tagger import PosTagger\n",
    "from attention_extraction import extract_all_attention, get_causal_selfattention_pattern\n",
    "from data_handling import load_tinystories_data\n",
    "from plotting import plot_probe_results_from_tag, plot_idx_of_highest_output, plot_selfattention_from_idx\n",
    "\n",
    "data = load_tinystories_data('../data/tinystories_val.txt')\n",
    "\n",
    "\n",
    "model_url = 'roneneldan/TinyStories-1M'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_url, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_url)\n",
    "\n",
    "pos_tagger = PosTagger(tokenizer)\n",
    "\n",
    "with open('../probe-results/results_keys.json', 'r') as file:\n",
    "    results_key = json.load(file) \n",
    "\n",
    "with open('../probe-results/results_queries.json', 'r') as file:\n",
    "    results_query = json.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../figures/case_studies/but_heads/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## head active after but\n",
    "\n",
    "layer 6, head 13 is active after a \"but\" and keeps attending back to it. Notably the probes did now exhibit an ability to identify conjunctions. Maybe this is because conjunctions include a lot of other words than \"but\" and this head is more specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 6\n",
    "head = 13\n",
    "input = data[50]\n",
    "\n",
    "plot_probe_results_from_tag(results_key, 'CC', cmap='Blues', outfile=folder + 'but_CC_probe.pdf')\n",
    "\n",
    "top_idx, _, _ = plot_idx_of_highest_output(model, tokenizer, input, layer, head, pos_tagger, color=(0.5, 0.1, 0.1), outfile=folder + 'but_activity.pdf')\n",
    "\n",
    "keys, queries, values = extract_all_attention(model, tokenizer, input)\n",
    "attention = get_causal_selfattention_pattern(keys[layer][head], queries[layer][head])\n",
    "tokens, tags, words = pos_tagger.tag_input(input, return_words=True)\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(10, 1, figsize=(5,8), sharex=True)\n",
    "axs = axs.flatten()\n",
    "top_idx = top_idx - 5\n",
    "start_buffer = 5\n",
    "start = top_idx - start_buffer\n",
    "\n",
    "\n",
    "for idx, ax in enumerate(axs):\n",
    "    i = idx\n",
    "    idx += top_idx\n",
    "\n",
    "    # ax.set_title(words[idx])\n",
    "\n",
    "    ax.text(1.05, 0.5, words[idx], transform=ax.transAxes, rotation=0, \n",
    "            fontsize=12, va=\"center\", ha=\"left\")\n",
    "\n",
    "    attn_context = attention[idx, start : top_idx + 10]\n",
    "    words_context = words[start : top_idx + 10]\n",
    "    tags_context = tags[start : top_idx + 10]\n",
    "\n",
    "    barplot = [attn_context[j].item() for j in range(10 + start_buffer)]\n",
    "\n",
    "    bars = ax.bar(range(start, top_idx + 10), barplot, color=(0.1, 0.2, 0.5))\n",
    "    bars[i + start_buffer].set_color((0.4, 0.6, 0.8))\n",
    "    ax.set_ylim((0, 0.8))\n",
    "    ax.set_xticks(range(start, top_idx + 10))\n",
    "    ax.set_xticklabels([f'{word} ({tag})' for word, tag in zip(words_context, tags_context)], rotation=45, ha='right')\n",
    "\n",
    "fig.savefig(folder + 'but_attention.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from pos_tagger import PosTagger\n",
    "from data_handling import load_tinystories_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from attention_extraction import extract_all_attention, get_causal_selfattention_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import plotting\n",
    "\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../probe-results/results_queries.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(data[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = len(data)\n",
    "heads = len(data[0])\n",
    "postags = tags[:tags.index('accuracy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5][13]['PRP$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_max = {}\n",
    "\n",
    "for layer in range(layers):\n",
    "    max_head, max_accuracy = max(((head, data[layer][head]['accuracy']) for head in range(heads)),key=lambda x: x[1])\n",
    "    \n",
    "    layer_max[layer] = {'head': f'{layer}_{max_head}', 'accuracy': max_accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_max_head = {}  \n",
    "\n",
    "for tag in postags:  \n",
    "\n",
    "    max_layer, max_head, max_f1 = max(((layer, head, data[layer][head][tag]['f1-score']) for layer in range(layers) for head in range(heads)),key=lambda x: x[2])\n",
    "    \n",
    "    tag_max_head[tag] = {\n",
    "        'head': f'{max_layer}_{max_head}',  \n",
    "        'f1-score': max_f1  \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_max_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'roneneldan/TinyStories-1M'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_url,output_attentions = True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_url)\n",
    "pos_tagger = PosTagger(tokenizer)\n",
    "\n",
    "sentences = load_tinystories_data('../data/tinystories_val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_averaged_queries(attentions, layer, head, lookback, tag, tags_train, all_tags):\n",
    "    averaged_attention = {tag: {inner_tag: [] for inner_tag in all_tags}}\n",
    "\n",
    "    for index in range(len(tags_train)):\n",
    "        if tags_train[index] == tag:\n",
    "            before = max(index - lookback, 0)  # Ensure `before` is not negative\n",
    "            for i in range(before, index):\n",
    "                inner_tag = tags_train[i]\n",
    "                value = attentions[layer][0][head][index][i].detach().numpy()\n",
    "                averaged_attention[tag][inner_tag].append(value)\n",
    "\n",
    "    for inner_tag, value_list in averaged_attention[tag].items():\n",
    "        if value_list:  # Check if the list is not empty\n",
    "            averaged_attention[tag][inner_tag] = np.mean(value_list)\n",
    "        else:\n",
    "            averaged_attention[tag][inner_tag] = 0  # or another default value\n",
    "\n",
    "    # Visualization with consistent x-axis\n",
    "    inner_dict = averaged_attention[tag]\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Ensure consistent ordering of bars\n",
    "    ordered_values = [inner_dict.get(inner_tag, 0) for inner_tag in all_tags]\n",
    "    ax.bar(x=all_tags, height=ordered_values,color = 'darkblue')\n",
    "\n",
    "    ax.grid(axis='y', linestyle='--', color='gray', alpha=0.7)\n",
    "    ax.set_xticks(range(len(all_tags)))\n",
    "    ax.set_xticklabels(all_tags, rotation=90, ha='center')  # Rotate for readability\n",
    "\n",
    "    plt.title(f\"Averaged Attention Values from {tag}\")\n",
    "    plt.ylabel(\"Average Attention Value\")\n",
    "    plt.xlabel(\"POS Tags\")\n",
    "    plt.show()\n",
    "\n",
    "    return averaged_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    inputs = tokenizer(sentences[i], return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  \n",
    "    tags_train = pos_tagger.tag_input(sentences[i])\n",
    "    ls = {tags_train[1][j] : tokenizer.decode(tags_train[0]['input_ids'][j]) for j in range(len(tags_train[1]))}\n",
    "    tag = 'PRP'\n",
    "\n",
    "    tags_vb = ['VB' if tag.startswith('VB') else tag for tag in tags_train[1]]\n",
    "\n",
    "    pos = [i for i in range(len(tags_train[1])) if tags_train[1][i] == tag]\n",
    "    extract_averaged_queries(layer=2, head=6, lookback=40,tag = tag, tags_train=tags_vb,attentions=attentions,all_tags=postags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries(index, layer, head, lookback, tag):\n",
    "    dit = dict()\n",
    "    before = index - lookback\n",
    "    if before >= 0:\n",
    "        for i in range(before, index):\n",
    "            if i != index:\n",
    "                outer_tag = tags_train[1][index]  # Assuming tags_train is a list or similar structure\n",
    "                inner_tag = tags_train[1][i]\n",
    "                if outer_tag not in dit:\n",
    "                    dit[outer_tag] = {}\n",
    "                if inner_tag not in dit[outer_tag]:\n",
    "                    dit[outer_tag][inner_tag] = attentions[layer][0][head][index][i].detach().cpu().numpy()  # Ensure tensor is on CPU before calling numpy()\n",
    "                else:\n",
    "                    # Take the mean of the previous values and the current attention score\n",
    "                    dit[outer_tag][inner_tag] = np.mean([dit[outer_tag][inner_tag], attentions[layer][0][head][index][i].detach().cpu().numpy()])\n",
    "\n",
    "    all_tags = list(set(tags_train[1]))  # Unique tags in tags_train\n",
    "    current_data = dit.get(tags_train[1][index], {})\n",
    "    heights = [current_data.get(tag, 0) for tag in all_tags]  # Use 0 for missing tags\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(x=all_tags, height=heights, color='darkblue')\n",
    "    plt.grid(axis='y', linestyle='--', color='gray', alpha=0.7)\n",
    "    plt.title(f\"Averaged Attention Values from {tag}\")\n",
    "    plt.xticks(range(len(all_tags)), all_tags, rotation=90, ha='center')\n",
    "    plt.ylabel(\"Average Attention\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filter = sentences[1530]\n",
    "inputs = tokenizer(data_filter, return_tensors=\"pt\")\n",
    "tags_train = pos_tagger.tag_input(data_filter, return_words = True)\n",
    "outputs = model(**inputs)\n",
    "attentions = outputs.attentions\n",
    "tag = tags_train[1][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_queries(24,5,5,24,tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_selfattention_from_idx(attention=attentions[5][0][5],tags=tags_train[1],idx = 24, context_size=24, words = tags_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_averaged_across_sentences(layer, head, lookback, tag, data, tokenizer, model, pos_tagger):\n",
    "\n",
    "    cumulative_attention = dict()\n",
    "    all_tags_set = set()\n",
    "    tag_counts = Counter() \n",
    "\n",
    "    for sentence in data:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        attentions = outputs.attentions  \n",
    "\n",
    "        tags_train = pos_tagger.tag_input(sentence)\n",
    "        sentence_tags = tags_train[1]\n",
    "        all_tags_set.update(sentence_tags)\n",
    "        tag_counts.update(sentence_tags)  \n",
    "\n",
    "        if not cumulative_attention:\n",
    "            cumulative_attention = {tag: [] for tag in all_tags_set}\n",
    "\n",
    "        for index, current_tag in enumerate(sentence_tags):\n",
    "            if current_tag == tag:\n",
    "                start = max(0, index - lookback)\n",
    "                for i in range(start, index):\n",
    "                    if i != index:\n",
    "                        context_tag = sentence_tags[i]\n",
    "                        value = attentions[layer][0][head][index][i].detach().cpu().numpy()\n",
    "                        cumulative_attention.setdefault(context_tag, []).append(value)\n",
    "\n",
    "    averaged_attention = {\n",
    "        key: (np.mean(values) if values else 0)\n",
    "        for key, values in cumulative_attention.items()\n",
    "    }\n",
    "\n",
    "    sorted_tags = sorted(all_tags_set)\n",
    "    sorted_values = [averaged_attention.get(tag, 0) for tag in sorted_tags]\n",
    "\n",
    "    max_count = max(tag_counts.values())\n",
    "    alphas = [(tag_counts[tag] / max_count) if tag in tag_counts else 0 for tag in sorted_tags]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    bars = plt.bar(sorted_tags, sorted_values, color='darkblue', alpha=1.0)  # Default alpha\n",
    "\n",
    "    for bar, alpha in zip(bars, alphas):\n",
    "        bar.set_alpha(alpha)\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=90, ha='center')\n",
    "    plt.title(f\"Averaged Attention from {tag}\")\n",
    "    plt.ylabel(\"Average Attention\")\n",
    "    plt.xlabel(\"POS Tags\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return averaged_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_attention_result = extract_averaged_across_sentences(\n",
    "    layer=5, \n",
    "    head=5, \n",
    "    lookback=40, \n",
    "    tag='PRP$', \n",
    "    data=sentences[:100], \n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    pos_tagger=pos_tagger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_relation(attention, tags, key_tag, context_size):\n",
    "    key_tag_mask = np.array(tags) == key_tag\n",
    "    avgs = defaultdict(list)\n",
    "\n",
    "    for idx, tag in enumerate(tags):\n",
    "        if idx < context_size:\n",
    "            continue\n",
    "        att_window = attention[idx, max(0, idx - context_size) : idx]\n",
    "        mask_windows = key_tag_mask[max(0, idx - context_size) : idx]\n",
    "\n",
    "        if sum(mask_windows):\n",
    "            avgs[tag].append(att_window[mask_windows].sum().item())\n",
    "\n",
    "    for tag in avgs:\n",
    "        avgs[tag] = mean(avgs[tag])\n",
    "\n",
    "    return dict(avgs)\n",
    "\n",
    "def plot_avg_attention(data, model, tokenizer, layer=5, head=5, tag='PRP$', context_size=40):\n",
    "    avg_attention_aggr = defaultdict(list)\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    # Process the sentences to get attention relations\n",
    "    for input in data:\n",
    "        pos_tagger = PosTagger(tokenizer)\n",
    "        tokens, tags, words = pos_tagger.tag_input(input, return_words=True)\n",
    "        tokenized = tokenizer(input, return_tensors='pt')\n",
    "        attentions = model(tokenized.input_ids).attentions\n",
    "        attention = attentions[layer][0][head]\n",
    "\n",
    "        avg_attention = get_tag_relation(attention, tags, key_tag=tag, context_size=context_size)\n",
    "\n",
    "        # Aggregate the attention results and tag counts\n",
    "        for key in avg_attention:\n",
    "            avg_attention_aggr[key].append(avg_attention[key])\n",
    "            tag_counts[key] += 1\n",
    "\n",
    "    # Calculate mean and length for each tag\n",
    "    for key in avg_attention_aggr:\n",
    "        avg_attention_aggr[key] = (mean(avg_attention_aggr[key]), len(avg_attention_aggr[key]))\n",
    "\n",
    "    # Sort tags alphabetically\n",
    "    sorted_tags = sorted(avg_attention_aggr.keys())\n",
    "    sorted_values = [avg_attention_aggr.get(tag, (0, 0))[0] for tag in sorted_tags]\n",
    "\n",
    "    # Normalize alpha values based on tag counts\n",
    "    max_count = max(tag_counts.values())\n",
    "    alphas = [(tag_counts[tag] / max_count) if tag in tag_counts else 0 for tag in sorted_tags]\n",
    "\n",
    "    # Create the figure for plotting\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Create the bars with a default color and alpha\n",
    "    bars = plt.bar(sorted_tags, sorted_values, color='darkblue', alpha=1.0)\n",
    "\n",
    "    # Adjust alpha based on tag counts\n",
    "    for bar, alpha in zip(bars, alphas):\n",
    "        bar.set_alpha(alpha)\n",
    "\n",
    "    # Add gridlines to the y-axis\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=90, ha='center')\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title(f\"Averaged Attention to {tag}\")\n",
    "    plt.ylabel(\"Average Attention\")\n",
    "    plt.xlabel(\"POS Tags\")\n",
    "\n",
    "    # Ensure the plot is well-organized\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    return avg_attention_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_key_attention_result = plot_avg_attention(\n",
    "    layer = 5,\n",
    "    head = 5,\n",
    "    context_size = 40,\n",
    "    tag = 'NN',\n",
    "    data = sentences[:100],\n",
    "    tokenizer = tokenizer, \n",
    "    model = model\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from data_handling import load_tinystories_data\n",
    "from attention_extraction import plot_selfattention_pattern, extract_all_attention\n",
    "from pos_tagger import PosTagger\n",
    "\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_url = 'roneneldan/TinyStories-1M'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_url, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_url)\n",
    "\n",
    "\n",
    "data = load_tinystories_data('data/tinystories_val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_relation(attention, tags, key_tag, context_size):\n",
    "\n",
    "\n",
    "    key_tag_mask = np.array(tags) == key_tag\n",
    "\n",
    "    avgs = defaultdict(list)\n",
    "\n",
    "    for idx, tag in enumerate(tags):\n",
    "        if idx < context_size:\n",
    "            continue\n",
    "        att_window = attention[idx, max(0, idx - context_size) : idx]\n",
    "\n",
    "        mask_windows = key_tag_mask[max(0, idx - context_size) : idx]\n",
    "\n",
    "        if sum(mask_windows):\n",
    "            avgs[tag].append(att_window[mask_windows].sum().item())\n",
    "\n",
    "\n",
    "    for tag in avgs:\n",
    "        avgs[tag] = mean(avgs[tag])\n",
    "\n",
    "    return dict(avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "head = 11\n",
    "tag = 'DT'\n",
    "\n",
    "avg_attention_aggr = defaultdict(list)\n",
    "\n",
    "for input in data[:100]:\n",
    "    pos_tagger = PosTagger(tokenizer)\n",
    "    tokens, tags, words = pos_tagger.tag_input(input, return_words=True)\n",
    "    tokenized = tokenizer(input, return_tensors='pt')\n",
    "    attentions = model(tokenized.input_ids).attentions\n",
    "    attention = attentions[layer][0][head]\n",
    "\n",
    "    avg_attention = get_tag_relation(attention, tags, key_tag=tag, context_size=40)\n",
    "\n",
    "    for key in avg_attention:\n",
    "        avg_attention_aggr[key].append(avg_attention[key])\n",
    "\n",
    "for key in avg_attention_aggr:\n",
    "    avg_attention_aggr[key] = (mean(avg_attention_aggr[key]), len(avg_attention_aggr[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15,3))\n",
    "\n",
    "max_val = max([len_ for _, len_ in avg_attention_aggr.values()])\n",
    "means, lens = zip(*avg_attention_aggr.values())\n",
    "bars = ax.bar(avg_attention_aggr.keys(), means)\n",
    "\n",
    "for bar, alpha in zip(bars, lens):\n",
    "    bar.set_alpha(alpha / max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = data[100]\n",
    "pos_tagger = PosTagger(tokenizer)\n",
    "tokens, tags, words = pos_tagger.tag_input(input, return_words=True)\n",
    "tokenized = tokenizer(input, return_tensors='pt')\n",
    "attentions = model(tokenized.input_ids).attentions\n",
    "attention = attentions[layer][0][head]\n",
    "\n",
    "start, end = (0,7)\n",
    "\n",
    "plot_selfattention_pattern(attention[start:end, start:end], words[start:end], tags[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions[2][0][6][64][63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tag_ in enumerate(tags):\n",
    "    if tag_ == tag: \n",
    "        print(tag_, idx)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tag_ in enumerate(tags):\n",
    "    if tag_ == 'VBD': \n",
    "        print(tag_, idx)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention[24, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ = 'Once upon a time, a little boy named Florian saw'\n",
    "tokenized = tokenizer(string_, return_tensors='pt')\n",
    "\n",
    "output = model.generate(tokenized.input_ids, max_length=300)\n",
    "\n",
    "print(tokenizer.decode(output[0][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_tag ='EX'\n",
    "\n",
    "key_tag_mask = np.array(tags) == key_tag\n",
    "\n",
    "avgs = defaultdict(list)\n",
    "\n",
    "for idx, tag in enumerate(tags):\n",
    "    att_window = attention[idx, max(0, idx - 30) : idx]\n",
    "    mask_windows = key_tag_mask[max(0, idx - 30) : idx]\n",
    "    if sum(mask_windows):\n",
    "        avgs[tag].append(att_window[mask_windows].mean().item())\n",
    "\n",
    "\n",
    "for tag in avgs:\n",
    "    avgs[tag] = mean(avgs[tag])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 2\n",
    "head = 7\n",
    "\n",
    "get_tag_relation(attentions[layer][0][head], tags, 'PRP$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_selfattention_pattern(attentions[layer][0][head], words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "att_for_head = output.attentions[layer][0][head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tag_relation(att_for_head, tags, 'DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
